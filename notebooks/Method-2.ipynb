{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Method2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abvoo72jWwQn",
        "outputId": "ae24d064-692b-421b-e016-44b006408285"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Oj2Fh5W0ej",
        "outputId": "4bcebf29-b1f5-46b4-ae8e-55c15462f31f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# from textblob import TextBlob\n",
        "\n",
        "path = \"/content/gdrive/MyDrive/enter_weights/\"\n",
        "df = pd.read_csv(path+\"clean_train.csv\")\n",
        "df_test = pd.read_csv(path+\"clean_test.csv\")\n",
        "\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  ...  clean_aspect\n",
              "0    can you check whether its cancelled completely?  ...      canceled\n",
              "1  cannot rely on both milk delivery and grocery ...  ...          milk\n",
              "2  I get no notification, however the app is real...  ...  ratification\n",
              "3  Love this app, but would love it even more if ...  ...          view\n",
              "4        it does not let me load a clip on the scene  ...          load\n",
              "\n",
              "[5 rows x 5 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>aspect</th>\n",
              "      <th>label</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>clean_aspect</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>can you check whether its cancelled completely?</td>\n",
              "      <td>cancelled</td>\n",
              "      <td>1</td>\n",
              "      <td>can you check whether its canceled completely?</td>\n",
              "      <td>canceled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cannot rely on both milk delivery and grocery ...</td>\n",
              "      <td>Milk</td>\n",
              "      <td>0</td>\n",
              "      <td>cannot rely on both milk delivery and grocer i...</td>\n",
              "      <td>milk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I get no notification, however the app is real...</td>\n",
              "      <td>notification</td>\n",
              "      <td>0</td>\n",
              "      <td>i get no ratification, however the pp is reall...</td>\n",
              "      <td>ratification</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Love this app, but would love it even more if ...</td>\n",
              "      <td>view</td>\n",
              "      <td>1</td>\n",
              "      <td>love this pp, but would love it even more if w...</td>\n",
              "      <td>view</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it does not let me load a clip on the scene</td>\n",
              "      <td>load</td>\n",
              "      <td>0</td>\n",
              "      <td>it does not let me load a clip on the scene</td>\n",
              "      <td>load</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "00AdGTbnW84v",
        "outputId": "f1161d67-d207-4b6c-a500-4fb2c344e5c2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "punctuation = '!\"#$%()*+-/:;<=>@[\\\\]^_`{|}~'\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x: \"\".join(ch for ch in x if ch not in set(punctuation)))\n",
        "df[\"clean_text\"] = df[\"clean_text\"].str.lower()\n",
        "df[\"clean_text\"] = df[\"clean_text\"].str.replace(\"[0-9]\", \" \")\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x:\" \".join(x.split()))\n",
        "# df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "\n",
        "df[\"clean_aspect\"] = df[\"aspect\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "df[\"clean_aspect\"] = df[\"clean_aspect\"].apply(lambda x: \"\".join(ch for ch in x if ch not in set(punctuation)))\n",
        "df[\"clean_aspect\"] = df[\"clean_aspect\"].str.lower()\n",
        "df[\"clean_aspect\"] = df[\"clean_aspect\"].str.replace(\"[0-9]\", \" \")\n",
        "df[\"clean_aspect\"] = df[\"clean_aspect\"].apply(lambda x:\" \".join(x.split()))\n",
        "# df[\"clean_aspect\"] = df[\"clean_aspect\"].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "\n",
        "df_test[\"clean_text\"] = df_test[\"text\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "df_test[\"clean_text\"] = df_test[\"clean_text\"].apply(lambda x: \"\".join(ch for ch in x if ch not in set(punctuation)))\n",
        "df_test[\"clean_text\"] = df_test[\"clean_text\"].str.lower()\n",
        "df_test[\"clean_text\"] = df_test[\"clean_text\"].str.replace(\"[0-9]\", \" \")\n",
        "df_test[\"clean_text\"] = df_test[\"clean_text\"].apply(lambda x:\" \".join(x.split()))\n",
        "# df_test[\"clean_text\"] = df_test[\"clean_text\"].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "\n",
        "df_test[\"clean_aspect\"] = df_test[\"aspect\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "df_test[\"clean_aspect\"] = df_test[\"clean_aspect\"].apply(lambda x: \"\".join(ch for ch in x if ch not in set(punctuation)))\n",
        "df_test[\"clean_aspect\"] = df_test[\"clean_aspect\"].str.lower()\n",
        "df_test[\"clean_aspect\"] = df_test[\"clean_aspect\"].str.replace(\"[0-9]\", \" \")\n",
        "df_test[\"clean_aspect\"] = df_test[\"clean_aspect\"].apply(lambda x:\" \".join(x.split()))\n",
        "# df_test[\"clean_aspect\"] = df_test[\"clean_aspect\"].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "\n",
        "# df.to_csv(path+\"clean_train.csv\", index=False)\n",
        "# df_test.to_csv(path+\"clean_test.csv\", index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "FnQESnrfbwyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def build_k_datasets(texts, phrases, targets, splits=5, random_state=0):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts: np.array[(str)] or List[str]\n",
        "      A array of texts\n",
        "    phrases: np.array[(str)] or List[str]\n",
        "      A array of phrases\n",
        "    splits: int\n",
        "      Number of splits\n",
        "    random_state: int\n",
        "      Random state for reprodubility\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    train_dataset: List[List[str]]\n",
        "        A k fold train split of strings in this format (Texts_array, Phrase_array, targets_array)\n",
        "    test_dataset: List[List[str]]\n",
        "        A k fold test split of strings in this format (Texts_array, Phrase_array, targets_array)\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=splits, shuffle=True,\n",
        "                          random_state=random_state)\n",
        "\n",
        "    if type(texts) != np.ndarray:\n",
        "        texts = np.array(texts)\n",
        "    if type(phrases) != np.ndarray:\n",
        "        phrases = np.array(phrases)\n",
        "    if type(targets) != np.ndarray:\n",
        "        targets = np.array(targets)\n",
        "\n",
        "    skf.get_n_splits(texts, targets)\n",
        "    train_dataset = []\n",
        "    test_dataset = []\n",
        "    for train_index, test_index in skf.split(texts, targets):\n",
        "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "        text_train, text_test = texts[train_index], texts[test_index]\n",
        "        phrase_train, phrase_test = phrases[train_index], phrases[test_index]\n",
        "        targets_train, targets_test = targets[train_index], targets[test_index]\n",
        "\n",
        "        train_dataset.append((text_train, phrase_train, targets_train))\n",
        "        test_dataset.append((text_test, phrase_test, targets_test))\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "swnwwo7Le2Sd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from logging import error\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class BERT_Dataset_Type_1:\n",
        "    def __init__(self,\n",
        "                 texts,\n",
        "                 phrases,\n",
        "                 targets,\n",
        "                 tokenizer_save_path,\n",
        "                 tokenizer_method=BertTokenizerFast,\n",
        "                 max_len=128):\n",
        "        self.texts = texts\n",
        "        self.phrases = phrases\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer_method.from_pretrained(\n",
        "            tokenizer_save_path,\n",
        "            do_lower_case=True,\n",
        "        )\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        # print(item)\n",
        "        # Calling the indexes of texts and phrases\n",
        "        text = self.texts[item]\n",
        "        phrase = self.phrases[item]\n",
        "        text = \" \".join(text.split()).lower()\n",
        "        phrase = \" \".join(phrase.split()).lower()\n",
        "        # print(text, len(text))\n",
        "        # print(phrase)\n",
        "        # To find entries of phrases in text\n",
        "        # Example text = \"I am a monkey\" , phrase = \"monkey\"\n",
        "        # char_mask = [0,0,0,0,0,0,0,1,1,1,1,1,1]\n",
        "        char_mask = [(m.start(0), m.end(0))\n",
        "                     for m in re.finditer(phrase.lower(), text.lower())]\n",
        "        tok = self.tokenizer(text, return_offsets_mapping=True)\n",
        "        # print(char_mask)\n",
        "        tok_ids = tok.input_ids\n",
        "        tok_offsets = tok.offset_mapping\n",
        "        buffer = [0] * len(text)\n",
        "        token_type_ids = [0]*(len(tok_ids)-2)\n",
        "        # print(\"tok ids\", tok_ids, len(tok_ids))\n",
        "        # print(\"token type ids\", token_type_ids, len(token_type_ids))\n",
        "        # print(self.tokenizer.encode(text).tokens)\n",
        "        # For each token in the phrase, we represent it with 1\n",
        "        # text = \"I am a monkey\"\n",
        "        # token_type_ids = [0, 0, 0, 1]\n",
        "        for m in char_mask:\n",
        "            for i in range(m[0], m[1]):\n",
        "                buffer[i] = 1\n",
        "\n",
        "        for j, (offset1, offset2) in enumerate(tok_offsets):\n",
        "            if sum(buffer[offset1:offset2]) > 0 and j < len(tok_ids)-2:\n",
        "                token_type_ids[j] = 1\n",
        "        \n",
        "        if sum(token_type_ids) is not 0:\n",
        "          token_type = [0] + token_type_ids + [0]\n",
        "        else:\n",
        "          token_type = [1] + token_type_ids + [0]\n",
        "\n",
        "        tok_ids = torch.tensor(tok_ids, dtype=torch.long)\n",
        "        tok_masks = torch.tensor(tok.attention_mask, dtype=torch.long)\n",
        "        tok_type = torch.tensor(token_type, dtype=torch.long)\n",
        "        targets = torch.tensor(self.targets[item], dtype=torch.float)\n",
        "        # print(tok_ids.shape, tok_masks.shape, tok_type.shape, targets)\n",
        "        return tok_ids, tok_masks, tok_type, targets\n",
        "\n",
        "\n",
        "def custom_collate(data):\n",
        "    max_len = 0\n",
        "    ids, masks, tok_types, targets = map(list, zip(*data))\n",
        "    for id in ids:\n",
        "        # print(id.shape[0])\n",
        "        max_len = max(max_len, id.shape[0])\n",
        "    \n",
        "    padded_ids, padded_masks, padded_tok_types = [], [], []\n",
        "    \n",
        "    for id in ids:\n",
        "        pad_size = max_len - id.shape[0]\n",
        "        padded_ids.append(F.pad(id, (0, pad_size), \"constant\", 0))\n",
        "\n",
        "    for mask in masks:\n",
        "        pad_size = max_len - mask.shape[0]\n",
        "        padded_masks.append(F.pad(mask, (0, pad_size), \"constant\", 0))\n",
        "\n",
        "    for tok_type in tok_types:\n",
        "        pad_size = max_len - tok_type.shape[0]\n",
        "        padded_tok_types.append(F.pad(tok_type, (0, pad_size), \"constant\", 0))\n",
        "\n",
        "    padded_ids = torch.stack(padded_ids, dim=0)\n",
        "    padded_masks = torch.stack(padded_masks, dim=0)\n",
        "    padded_tok_types = torch.stack(padded_tok_types, dim=0)\n",
        "    targets = torch.stack(targets, dim=0)\n",
        "    return padded_ids, padded_masks, padded_tok_types, targets\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "bQVJk87ne2Y5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import AutoModel\n",
        "class Embedding_model(nn.Module):\n",
        "    def __init__(self, BERT_PATH, base_model_dim):\n",
        "        super(Embedding_model, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(BERT_PATH)\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.head1 = nn.Linear(base_model_dim, 256)\n",
        "        self.head2 = nn.Linear(256, 3)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, ids, attn_mask, token_types):\n",
        "        bert_output = self.bert(ids, attn_mask)\n",
        "        embed_extract = []\n",
        "        for bert_embed, token_type in zip(bert_output[0], token_types):\n",
        "            embed_extract.append(torch.mean(bert_embed[token_type == 1], axis=0))\n",
        "\n",
        "        embed_extract = torch.stack(embed_extract, dim=0)\n",
        "        output = self.head1(embed_extract)\n",
        "        output = F.leaky_relu(self.dropout(output))\n",
        "        output = self.head2(output)\n",
        "        return output\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "q81cFXH9YOc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def classify_default(model, tok_ids, attn_mask, tok_type, device):\n",
        "\n",
        "    tok_ids = tok_ids.to(device)\n",
        "    attn_mask = attn_mask.to(device)\n",
        "    tok_type = tok_type.to(device)\n",
        "    return model(tok_ids,\n",
        "                 attn_mask,\n",
        "                 tok_type)\n",
        "\n",
        "\n",
        "def loss_fn_cross_entropy(outputs, targets):\n",
        "    return nn.CrossEntropyLoss()(outputs, targets.to(int))\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Will be used for Trainer Class for training the model\n",
        "\n",
        "    1. Will train the model\n",
        "    2. Will evaluate the model\n",
        "    3. Will save the torch model\n",
        "    4. Will save the model to onnx\n",
        "    5. Can connect it to wandb\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 classify,\n",
        "                 config,\n",
        "                 device=None\n",
        "                 ):\n",
        "        if device is not None:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = torch.device(\n",
        "                \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer = AdamW(model.parameters(),\n",
        "                               lr=config[\"learning_rate\"])\n",
        "        self.criterion = loss_fn_cross_entropy\n",
        "        self.classify = classify\n",
        "        self.config = config\n",
        "        num_train_steps = int(config[\"df_len\"] / config[\"train_batch_size\"] * config[\"num_epochs\"])\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer, num_warmup_steps=0,\n",
        "            num_training_steps=num_train_steps\n",
        "        )\n",
        "\n",
        "    def train_classification(self,\n",
        "                             train_dataloader,\n",
        "                             val_dataloader,\n",
        "                             start_step=0):\n",
        "\n",
        "        training_log = []\n",
        "\n",
        "        for epoch in range(self.config[\"num_epochs\"]):\n",
        "            start_time = time.time()\n",
        "            self.model.train()\n",
        "            running_loss = []\n",
        "\n",
        "            for batch_idx, batch in tqdm(enumerate(train_dataloader),\n",
        "                                         total=len(train_dataloader),\n",
        "                                         leave=False):\n",
        "                self.optimizer.zero_grad()\n",
        "                tok_ids, attn_mask, tok_type, targets = batch\n",
        "\n",
        "                predictions = classify(self.model, tok_ids, attn_mask, tok_type, self.device)\n",
        "                loss = self.criterion(predictions, targets.to(self.device))\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                start_step += 1\n",
        "\n",
        "            running_loss = np.mean(running_loss)\n",
        "            overall_acc, acc_1, acc_2, acc_3, f1, val_loss = self.get_validation(\n",
        "                val_dataloader)\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            training_log.append([epoch, running_loss, val_loss,\n",
        "                                 acc_1, acc_2, acc_3, f1])\n",
        "            print(\n",
        "                \"| epoch {:3d} | train loss {:8.5f} | Val loss {:8.5f} | Overall acc {:5.3f} | \"\n",
        "                \"class_1_acc {:5.3f} | class_2_acc {:5.3f} | class_3_acc {:5.3f} | \"\n",
        "                \"f1_score {:5.3f} | Time Taken {:5.3f}s |\".format(\n",
        "                    epoch, running_loss, val_loss, overall_acc,\n",
        "                    acc_1, acc_2, acc_3, f1, elapsed\n",
        "                )\n",
        "            )\n",
        "            print(\" \".join(\"-\" for _ in range(75)))\n",
        "\n",
        "        return training_log\n",
        "\n",
        "    def get_validation(self, val_dataloader):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_dataloader):\n",
        "                tok_ids, attn_mask, tok_type, targets = batch\n",
        "\n",
        "                predictions = classify(self.model, tok_ids, attn_mask, tok_type, self.device)\n",
        "                loss = self.criterion(\n",
        "                    predictions, targets.to(self.device))\n",
        "                predictions = torch.nn.functional.softmax(\n",
        "                    predictions, 1).detach().cpu()\n",
        "                predictions = torch.argmax(predictions, dim = 1)\n",
        "                running_loss.append(loss.item())\n",
        "                y_pred += predictions.tolist()\n",
        "                y_true += targets.cpu().tolist()\n",
        "\n",
        "        f1 = f1_score( y_true, y_pred, average=\"weighted\", zero_division=1)\n",
        "        overall_acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "        matrix = confusion_matrix(y_true, y_pred)\n",
        "        matrix = matrix.diagonal()/matrix.sum(axis=1)\n",
        "        return overall_acc, matrix[0], matrix[1], matrix[2], f1, np.mean(running_loss)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "fBugNOJi2BTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT study using Embedding vectors"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "config = {\n",
        "    \"splits\": 5,\n",
        "    \"seed\": 0,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"val_batch_size\": 64,\n",
        "    \"num_workers\": 2,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"df_len\": 3200,\n",
        "    \"num_epochs\": 7,\n",
        "    \"max_len\": 128,\n",
        "    \"checkpoint\": \"huawei-noah/TinyBERT_General_4L_312D\",\n",
        "    \"classify\": classify_default,\n",
        "    \"base_model_dim\": 312\n",
        "\n",
        "}\n",
        "texts_all = df[\"clean_text\"].to_numpy()\n",
        "phrases_all = df[\"clean_aspect\"].to_numpy()\n",
        "targets_all = df[\"label\"].to_numpy()\n",
        "\n",
        "train_dataset, test_dataset = build_k_datasets(texts_all, \n",
        "                                               phrases_all, \n",
        "                                               targets_all, \n",
        "                                               config[\"splits\"], \n",
        "                                               config[\"seed\"])\n",
        "\n",
        "for i, dataset in enumerate(zip(train_dataset, test_dataset)):\n",
        "    print(f\"Training for fold {i}\")\n",
        "    print(\" \".join(\"=\" for _ in range(75)))\n",
        "    checkpoint = config[\"checkpoint\"]\n",
        "    train, test = dataset\n",
        "    model = Embedding_model(checkpoint, config[\"base_model_dim\"])\n",
        "    texts, phrases, targets = train\n",
        "    val_texts, val_phrases, val_targets = test\n",
        "\n",
        "    train_dataset = BERT_Dataset_Type_1(texts, phrases, targets, checkpoint, tokenizer_method = AutoTokenizer, max_len=config[\"max_len\"])\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=config[\"train_batch_size\"], \n",
        "        num_workers=config[\"num_workers\"],\n",
        "        collate_fn = custom_collate\n",
        "    )\n",
        "\n",
        "    val_dataset = BERT_Dataset_Type_1(val_texts, val_phrases, val_targets, checkpoint, tokenizer_method = AutoTokenizer, max_len=config[\"max_len\"])\n",
        "    val_data_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=config[\"val_batch_size\"],\n",
        "        num_workers=config[\"num_workers\"],\n",
        "        collate_fn = custom_collate\n",
        "    )\n",
        "    engine = Trainer(model, config[\"classify\"], config)\n",
        "\n",
        "    log = engine.train_classification(\n",
        "        train_data_loader,\n",
        "        val_data_loader\n",
        "    )\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for fold 0\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.4.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.0.bias', 'cls.seq_relationship.bias', 'fit_denses.3.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.88074 | Val loss  0.78200 | Overall acc 0.652 | class_1_acc 0.792 | class_2_acc 0.610 | class_3_acc 0.478 | f1_score 0.647 | Time Taken 10.376s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.69127 | Val loss  0.72471 | Overall acc 0.690 | class_1_acc 0.815 | class_2_acc 0.587 | class_3_acc 0.615 | f1_score 0.686 | Time Taken 10.246s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.59751 | Val loss  0.69457 | Overall acc 0.708 | class_1_acc 0.708 | class_2_acc 0.718 | class_3_acc 0.693 | f1_score 0.708 | Time Taken 10.305s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.52220 | Val loss  0.72275 | Overall acc 0.693 | class_1_acc 0.649 | class_2_acc 0.722 | class_3_acc 0.727 | f1_score 0.693 | Time Taken 10.282s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.45834 | Val loss  0.75423 | Overall acc 0.703 | class_1_acc 0.714 | class_2_acc 0.633 | class_3_acc 0.771 | f1_score 0.703 | Time Taken 10.454s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   5 | train loss  0.41890 | Val loss  0.76212 | Overall acc 0.705 | class_1_acc 0.708 | class_2_acc 0.641 | class_3_acc 0.780 | f1_score 0.706 | Time Taken 10.277s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   6 | train loss  0.37993 | Val loss  0.73587 | Overall acc 0.708 | class_1_acc 0.732 | class_2_acc 0.668 | class_3_acc 0.717 | f1_score 0.708 | Time Taken 10.334s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 1\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.4.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.0.bias', 'cls.seq_relationship.bias', 'fit_denses.3.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.88526 | Val loss  0.76116 | Overall acc 0.706 | class_1_acc 0.842 | class_2_acc 0.707 | class_3_acc 0.483 | f1_score 0.699 | Time Taken 10.502s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.69645 | Val loss  0.71160 | Overall acc 0.724 | class_1_acc 0.827 | class_2_acc 0.726 | class_3_acc 0.551 | f1_score 0.720 | Time Taken 10.387s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.59070 | Val loss  0.68369 | Overall acc 0.735 | class_1_acc 0.801 | class_2_acc 0.707 | class_3_acc 0.663 | f1_score 0.734 | Time Taken 10.533s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.54062 | Val loss  0.69926 | Overall acc 0.726 | class_1_acc 0.720 | class_2_acc 0.718 | class_3_acc 0.746 | f1_score 0.728 | Time Taken 10.371s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.46335 | Val loss  0.71268 | Overall acc 0.730 | class_1_acc 0.699 | class_2_acc 0.745 | class_3_acc 0.761 | f1_score 0.731 | Time Taken 10.496s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   5 | train loss  0.39512 | Val loss  0.72632 | Overall acc 0.740 | class_1_acc 0.714 | class_2_acc 0.757 | class_3_acc 0.761 | f1_score 0.741 | Time Taken 10.592s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   6 | train loss  0.38014 | Val loss  0.71579 | Overall acc 0.735 | class_1_acc 0.777 | class_2_acc 0.699 | class_3_acc 0.712 | f1_score 0.735 | Time Taken 10.592s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 2\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.4.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.0.bias', 'cls.seq_relationship.bias', 'fit_denses.3.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.88481 | Val loss  0.72910 | Overall acc 0.695 | class_1_acc 0.750 | class_2_acc 0.753 | class_3_acc 0.532 | f1_score 0.694 | Time Taken 10.792s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.69285 | Val loss  0.71700 | Overall acc 0.713 | class_1_acc 0.845 | class_2_acc 0.718 | class_3_acc 0.488 | f1_score 0.706 | Time Taken 10.662s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.60286 | Val loss  0.69601 | Overall acc 0.719 | class_1_acc 0.827 | class_2_acc 0.710 | class_3_acc 0.551 | f1_score 0.715 | Time Taken 10.465s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.53480 | Val loss  0.69375 | Overall acc 0.726 | class_1_acc 0.780 | class_2_acc 0.668 | class_3_acc 0.712 | f1_score 0.726 | Time Taken 10.582s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.49004 | Val loss  0.68562 | Overall acc 0.714 | class_1_acc 0.705 | class_2_acc 0.726 | class_3_acc 0.712 | f1_score 0.716 | Time Taken 10.651s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   5 | train loss  0.42025 | Val loss  0.70316 | Overall acc 0.723 | class_1_acc 0.741 | class_2_acc 0.691 | class_3_acc 0.732 | f1_score 0.724 | Time Taken 10.564s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   6 | train loss  0.38545 | Val loss  0.68569 | Overall acc 0.726 | class_1_acc 0.768 | class_2_acc 0.726 | class_3_acc 0.659 | f1_score 0.726 | Time Taken 10.530s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 3\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.4.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.0.bias', 'cls.seq_relationship.bias', 'fit_denses.3.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.89325 | Val loss  0.82126 | Overall acc 0.656 | class_1_acc 0.812 | class_2_acc 0.606 | class_3_acc 0.463 | f1_score 0.649 | Time Taken 10.443s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.68914 | Val loss  0.78882 | Overall acc 0.694 | class_1_acc 0.839 | class_2_acc 0.606 | class_3_acc 0.566 | f1_score 0.689 | Time Taken 10.297s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.60084 | Val loss  0.76409 | Overall acc 0.690 | class_1_acc 0.771 | class_2_acc 0.606 | class_3_acc 0.663 | f1_score 0.688 | Time Taken 10.340s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.52138 | Val loss  0.76569 | Overall acc 0.695 | class_1_acc 0.670 | class_2_acc 0.718 | class_3_acc 0.707 | f1_score 0.696 | Time Taken 10.389s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.45333 | Val loss  0.78638 | Overall acc 0.698 | class_1_acc 0.699 | class_2_acc 0.695 | class_3_acc 0.698 | f1_score 0.698 | Time Taken 10.548s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   5 | train loss  0.40121 | Val loss  0.78756 | Overall acc 0.703 | class_1_acc 0.685 | class_2_acc 0.726 | class_3_acc 0.702 | f1_score 0.703 | Time Taken 10.428s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   6 | train loss  0.37031 | Val loss  0.79189 | Overall acc 0.703 | class_1_acc 0.735 | class_2_acc 0.668 | class_3_acc 0.693 | f1_score 0.703 | Time Taken 10.322s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 4\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.4.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.0.bias', 'cls.seq_relationship.bias', 'fit_denses.3.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.88891 | Val loss  0.76923 | Overall acc 0.665 | class_1_acc 0.789 | class_2_acc 0.674 | class_3_acc 0.451 | f1_score 0.658 | Time Taken 10.855s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.68833 | Val loss  0.72545 | Overall acc 0.704 | class_1_acc 0.771 | class_2_acc 0.744 | class_3_acc 0.544 | f1_score 0.702 | Time Taken 10.738s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.58317 | Val loss  0.73273 | Overall acc 0.711 | class_1_acc 0.714 | class_2_acc 0.740 | class_3_acc 0.670 | f1_score 0.712 | Time Taken 10.765s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.53494 | Val loss  0.74912 | Overall acc 0.698 | class_1_acc 0.690 | class_2_acc 0.702 | class_3_acc 0.704 | f1_score 0.699 | Time Taken 10.822s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.47132 | Val loss  0.74444 | Overall acc 0.710 | class_1_acc 0.693 | class_2_acc 0.783 | class_3_acc 0.646 | f1_score 0.711 | Time Taken 10.755s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   5 | train loss  0.41014 | Val loss  0.76037 | Overall acc 0.706 | class_1_acc 0.673 | class_2_acc 0.795 | class_3_acc 0.650 | f1_score 0.707 | Time Taken 10.702s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   6 | train loss  0.37692 | Val loss  0.74892 | Overall acc 0.716 | class_1_acc 0.750 | class_2_acc 0.733 | class_3_acc 0.641 | f1_score 0.716 | Time Taken 10.851s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU4wTOzt3mIr",
        "outputId": "e431c1f4-c1e9-459a-f29b-e2e6fe7121fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny BERT study using Embedding vectors"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "config = {\n",
        "    \"splits\": 5,\n",
        "    \"seed\": 0,\n",
        "    \"train_batch_size\": 8,\n",
        "    \"val_batch_size\": 16,\n",
        "    \"num_workers\": 2,\n",
        "    \"learning_rate\": 5e-6,\n",
        "    \"df_len\": 3200,\n",
        "    \"num_epochs\": 5,\n",
        "    \"max_len\": 128,\n",
        "    \"checkpoint\": \"bert-base-uncased\",\n",
        "    \"classify\": classify_default,\n",
        "    \"base_model_dim\": 768\n",
        "}\n",
        "texts_all = df[\"clean_text\"].to_numpy()\n",
        "phrases_all = df[\"clean_aspect\"].to_numpy()\n",
        "targets_all = df[\"label\"].to_numpy()\n",
        "\n",
        "train_dataset, test_dataset = build_k_datasets(texts_all, \n",
        "                                               phrases_all, \n",
        "                                               targets_all, \n",
        "                                               config[\"splits\"], \n",
        "                                               config[\"seed\"])\n",
        "\n",
        "for i, dataset in enumerate(zip(train_dataset, test_dataset)):\n",
        "    print(f\"Training for fold {i}\")\n",
        "    print(\" \".join(\"=\" for _ in range(75)))\n",
        "    checkpoint = config[\"checkpoint\"]\n",
        "    train, test = dataset\n",
        "    model = Embedding_model(checkpoint, config[\"base_model_dim\"])\n",
        "    texts, phrases, targets = train\n",
        "    val_texts, val_phrases, val_targets = test\n",
        "\n",
        "    train_dataset = BERT_Dataset_Type_1(texts, phrases, targets, checkpoint, tokenizer_method = AutoTokenizer, max_len=config[\"max_len\"])\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=config[\"train_batch_size\"], \n",
        "        num_workers=config[\"num_workers\"],\n",
        "        collate_fn = custom_collate\n",
        "    )\n",
        "\n",
        "    val_dataset = BERT_Dataset_Type_1(val_texts, val_phrases, val_targets, checkpoint, tokenizer_method = AutoTokenizer, max_len=config[\"max_len\"])\n",
        "    val_data_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=config[\"val_batch_size\"],\n",
        "        num_workers=config[\"num_workers\"],\n",
        "        collate_fn = custom_collate\n",
        "    )\n",
        "    engine = Trainer(model, config[\"classify\"], config)\n",
        "\n",
        "    log = engine.train_classification(\n",
        "        train_data_loader,\n",
        "        val_data_loader\n",
        "    )\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for fold 0\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.97646 | Val loss  0.78559 | Overall acc 0.645 | class_1_acc 0.711 | class_2_acc 0.602 | class_3_acc 0.590 | f1_score 0.644 | Time Taken 88.483s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.69468 | Val loss  0.71955 | Overall acc 0.680 | class_1_acc 0.735 | class_2_acc 0.664 | class_3_acc 0.610 | f1_score 0.680 | Time Taken 88.441s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.57811 | Val loss  0.70006 | Overall acc 0.699 | class_1_acc 0.735 | class_2_acc 0.683 | class_3_acc 0.659 | f1_score 0.699 | Time Taken 88.249s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.48971 | Val loss  0.70518 | Overall acc 0.718 | class_1_acc 0.753 | class_2_acc 0.649 | class_3_acc 0.746 | f1_score 0.717 | Time Taken 88.412s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.45473 | Val loss  0.70246 | Overall acc 0.709 | class_1_acc 0.753 | class_2_acc 0.664 | class_3_acc 0.693 | f1_score 0.708 | Time Taken 88.409s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 1\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.96386 | Val loss  0.77479 | Overall acc 0.684 | class_1_acc 0.798 | class_2_acc 0.710 | class_3_acc 0.463 | f1_score 0.677 | Time Taken 88.295s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.69468 | Val loss  0.68856 | Overall acc 0.714 | class_1_acc 0.777 | class_2_acc 0.761 | class_3_acc 0.551 | f1_score 0.711 | Time Taken 88.093s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.56252 | Val loss  0.67552 | Overall acc 0.730 | class_1_acc 0.792 | class_2_acc 0.726 | class_3_acc 0.634 | f1_score 0.729 | Time Taken 88.063s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.48721 | Val loss  0.67467 | Overall acc 0.729 | class_1_acc 0.759 | class_2_acc 0.718 | class_3_acc 0.693 | f1_score 0.729 | Time Taken 87.951s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.44215 | Val loss  0.67677 | Overall acc 0.729 | class_1_acc 0.771 | class_2_acc 0.718 | class_3_acc 0.673 | f1_score 0.729 | Time Taken 87.843s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 2\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.93843 | Val loss  0.74665 | Overall acc 0.696 | class_1_acc 0.792 | class_2_acc 0.699 | class_3_acc 0.537 | f1_score 0.693 | Time Taken 88.462s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.67451 | Val loss  0.66283 | Overall acc 0.726 | class_1_acc 0.824 | class_2_acc 0.672 | class_3_acc 0.634 | f1_score 0.724 | Time Taken 88.034s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.56194 | Val loss  0.64854 | Overall acc 0.738 | class_1_acc 0.789 | class_2_acc 0.714 | class_3_acc 0.683 | f1_score 0.738 | Time Taken 88.171s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.48488 | Val loss  0.66084 | Overall acc 0.744 | class_1_acc 0.780 | class_2_acc 0.683 | class_3_acc 0.761 | f1_score 0.744 | Time Taken 88.102s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.43925 | Val loss  0.64973 | Overall acc 0.757 | class_1_acc 0.818 | class_2_acc 0.726 | class_3_acc 0.698 | f1_score 0.757 | Time Taken 88.391s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 3\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.96939 | Val loss  0.80189 | Overall acc 0.664 | class_1_acc 0.738 | class_2_acc 0.653 | class_3_acc 0.556 | f1_score 0.662 | Time Taken 88.098s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.68651 | Val loss  0.75688 | Overall acc 0.680 | class_1_acc 0.774 | class_2_acc 0.653 | class_3_acc 0.561 | f1_score 0.678 | Time Taken 88.191s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.55813 | Val loss  0.73662 | Overall acc 0.691 | class_1_acc 0.762 | class_2_acc 0.637 | class_3_acc 0.644 | f1_score 0.691 | Time Taken 88.015s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.48397 | Val loss  0.74924 | Overall acc 0.691 | class_1_acc 0.750 | class_2_acc 0.614 | class_3_acc 0.693 | f1_score 0.691 | Time Taken 87.880s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.43550 | Val loss  0.76358 | Overall acc 0.690 | class_1_acc 0.756 | class_2_acc 0.622 | class_3_acc 0.668 | f1_score 0.689 | Time Taken 87.895s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "Training for fold 4\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   0 | train loss  0.98944 | Val loss  0.77851 | Overall acc 0.705 | class_1_acc 0.762 | class_2_acc 0.760 | class_3_acc 0.544 | f1_score 0.703 | Time Taken 88.566s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 | train loss  0.71681 | Val loss  0.66452 | Overall acc 0.728 | class_1_acc 0.798 | class_2_acc 0.760 | class_3_acc 0.573 | f1_score 0.726 | Time Taken 88.494s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   2 | train loss  0.58061 | Val loss  0.63379 | Overall acc 0.743 | class_1_acc 0.783 | class_2_acc 0.740 | class_3_acc 0.680 | f1_score 0.743 | Time Taken 88.455s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   3 | train loss  0.49271 | Val loss  0.64159 | Overall acc 0.733 | class_1_acc 0.750 | class_2_acc 0.725 | class_3_acc 0.714 | f1_score 0.734 | Time Taken 88.484s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   4 | train loss  0.44635 | Val loss  0.64572 | Overall acc 0.739 | class_1_acc 0.804 | class_2_acc 0.709 | class_3_acc 0.670 | f1_score 0.738 | Time Taken 88.370s |\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvuXrV5QCXk4",
        "outputId": "ba19b0e6-9bd8-4bad-c6bf-468a45993a5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "odIB8GA_DLhg"
      }
    }
  ]
}